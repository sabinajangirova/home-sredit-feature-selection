{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00187238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:27:57.520412Z",
     "iopub.status.busy": "2024-02-07T21:27:57.520013Z",
     "iopub.status.idle": "2024-02-07T21:28:01.736666Z",
     "shell.execute_reply": "2024-02-07T21:28:01.735326Z"
    },
    "papermill": {
     "duration": 4.227484,
     "end_time": "2024-02-07T21:28:01.739621",
     "exception": false,
     "start_time": "2024-02-07T21:27:57.512137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = Path('/home/sabina.jangirova/Documents/ML703_project/data')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b1f90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:01.755445Z",
     "iopub.status.busy": "2024-02-07T21:28:01.755022Z",
     "iopub.status.idle": "2024-02-07T21:28:01.764738Z",
     "shell.execute_reply": "2024-02-07T21:28:01.763428Z"
    },
    "papermill": {
     "duration": 0.020544,
     "end_time": "2024-02-07T21:28:01.767089",
     "exception": false,
     "start_time": "2024-02-07T21:28:01.746545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date,strict=False))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns: \n",
    "                if col.endswith(\"D\"):\n",
    "                    # Calculate the difference in days between each date column and date_decision\n",
    "                    df = df.with_columns(\n",
    "                        (pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(col)\n",
    "                    )\n",
    "                    df = df.with_columns(pl.col(col).fill_null(np.nan)) \n",
    "        # Drop date_decision column\n",
    "        df = df.drop(\"date_decision\")\n",
    "#         print(df.dtypes) # for Debugging\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df,base_df = None,test=False):\n",
    "        #for test data\n",
    "            for col in df.columns:\n",
    "                if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                    isnull = df[col].is_null().mean()\n",
    "                    if isnull > 0.7:\n",
    "                        df = df.drop(col)\n",
    "            columns_to_drop = []\n",
    "            for col in df.columns:\n",
    "                if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                    freq = df[col].n_unique()\n",
    "                    if (freq == 1) or (freq > 100):\n",
    "                        columns_to_drop.append(col)\n",
    "\n",
    "            df = df.drop(columns_to_drop)\n",
    "            return df\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs\n",
    "\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.set_table_dtypes)\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                try:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "                except:\n",
    "                    continue\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016b91f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:01.782683Z",
     "iopub.status.busy": "2024-02-07T21:28:01.782161Z",
     "iopub.status.idle": "2024-02-07T21:28:20.978046Z",
     "shell.execute_reply": "2024-02-07T21:28:20.977092Z"
    },
    "papermill": {
     "duration": 19.207089,
     "end_time": "2024-02-07T21:28:20.980847",
     "exception": false,
     "start_time": "2024-02-07T21:28:01.773758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8d8336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:20.996615Z",
     "iopub.status.busy": "2024-02-07T21:28:20.995823Z",
     "iopub.status.idle": "2024-02-07T21:28:21.064631Z",
     "shell.execute_reply": "2024-02-07T21:28:21.063597Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.079736,
     "end_time": "2024-02-07T21:28:21.067361",
     "exception": false,
     "start_time": "2024-02-07T21:28:20.987625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 2min 26s, total: 6min 44s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da15e81",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 488)\n",
      "Memory usage of dataframe is 3313.73 MB\n",
      "Memory usage after optimization is: 1097.80 MB\n",
      "Decreased by 66.9%\n",
      "train data shape:\t (1526659, 344)\n",
      "Use these ['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9']\n",
      "####### NAN count = 0\n",
      "####### NAN count = 1389663\n",
      "####### NAN count = 1411681\n",
      "####### NAN count = 1455026\n",
      "####### NAN count = 918788\n",
      "Use these ['dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D']\n",
      "####### NAN count = 140968\n",
      "####### NAN count = 1490159\n",
      "Use these ['pmtscount_423L', 'pmtssum_45A']\n",
      "####### NAN count = 954021\n",
      "####### NAN count = 806659\n",
      "####### NAN count = 866332\n",
      "####### NAN count = 1301747\n",
      "####### NAN count = 418178\n",
      "Use these ['amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L']\n",
      "####### NAN count = 561124\n",
      "Use these ['annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A']\n",
      "####### NAN count = 4\n",
      "Use these ['mindbddpdlast24m_3658935P']\n",
      "####### NAN count = 613202\n",
      "####### NAN count = 948244\n",
      "Use these ['mindbdtollast24m_4525191P']\n",
      "####### NAN count = 972827\n",
      "####### NAN count = 467175\n",
      "Use these ['avginstallast24m_3658937A', 'maxinstallast24m_3658928A']\n",
      "####### NAN count = 624875\n",
      "####### NAN count = 757006\n",
      "####### NAN count = 841181\n",
      "####### NAN count = 1026987\n",
      "####### NAN count = 455190\n",
      "####### NAN count = 460822\n",
      "Use these ['commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P']\n",
      "####### NAN count = 343375\n",
      "####### NAN count = 833735\n",
      "####### NAN count = 1392841\n",
      "####### NAN count = 887659\n",
      "Use these ['daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L']\n",
      "####### NAN count = 452594\n",
      "####### NAN count = 977119\n",
      "Use these ['eir_270L']\n",
      "####### NAN count = 190833\n",
      "####### NAN count = 859214\n",
      "####### NAN count = 482103\n",
      "####### NAN count = 453587\n",
      "Use these ['lastapplicationdate_877D', 'max_num_group1', 'max_num_group2_14']\n",
      "####### NAN count = 305137\n",
      "Use these ['lastapprcredamount_781A', 'lastapprdate_640D']\n",
      "####### NAN count = 442041\n",
      "####### NAN count = 977975\n",
      "Use these ['lastrejectcredamount_222A', 'lastrejectdate_50D']\n",
      "####### NAN count = 769046\n",
      "####### NAN count = 1524282\n",
      "####### NAN count = 511255\n",
      "Use these ['mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P']\n",
      "####### NAN count = 306019\n",
      "####### NAN count = 960953\n",
      "####### NAN count = 705504\n",
      "####### NAN count = 876276\n",
      "####### NAN count = 826000\n",
      "####### NAN count = 829402\n",
      "####### NAN count = 1032856\n",
      "####### NAN count = 766958\n",
      "Use these ['numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L']\n",
      "####### NAN count = 452593\n",
      "####### NAN count = 455081\n",
      "Use these ['numinstlsallpaid_934L']\n",
      "####### NAN count = 445669\n",
      "Use these ['numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L']\n",
      "####### NAN count = 456495\n",
      "Use these ['numinstpaid_4499208L']\n",
      "####### NAN count = 847191\n",
      "####### NAN count = 446983\n",
      "Use these ['numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A']\n",
      "####### NAN count = 840646\n",
      "####### NAN count = 669186\n",
      "####### NAN count = 455612\n",
      "####### NAN count = 1517330\n",
      "Use these ['pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L']\n",
      "####### NAN count = 458738\n",
      "####### NAN count = 461362\n",
      "####### NAN count = 459827\n",
      "####### NAN count = 460079\n",
      "####### NAN count = 44954\n",
      "####### NAN count = 78526\n",
      "####### NAN count = 131888\n",
      "####### NAN count = 181122\n",
      "####### NAN count = 223240\n",
      "####### NAN count = 445320\n",
      "####### NAN count = 3\n",
      "####### NAN count = 1374886\n",
      "####### NAN count = 305154\n",
      "####### NAN count = 308739\n",
      "Use these ['max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A']\n",
      "####### NAN count = 307441\n",
      "####### NAN count = 419006\n",
      "####### NAN count = 306361\n",
      "####### NAN count = 450969\n",
      "####### NAN count = 420383\n",
      "####### NAN count = 442999\n",
      "####### NAN count = 454678\n",
      "####### NAN count = 703840\n",
      "####### NAN count = 548987\n",
      "####### NAN count = 559169\n",
      "####### NAN count = 334873\n",
      "####### NAN count = 961606\n",
      "####### NAN count = 552766\n",
      "Use these ['max_pmtnum_8L']\n",
      "####### NAN count = 321446\n",
      "####### NAN count = 1068725\n",
      "####### NAN count = 1375927\n",
      "Use these ['max_pmtamount_36A', 'max_processingdate_168D', 'max_num_group1_5']\n",
      "####### NAN count = 1044394\n",
      "####### NAN count = 1036944\n",
      "####### NAN count = 603001\n",
      "Use these ['max_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T']\n",
      "####### NAN count = 263166\n",
      "Use these ['max_pmts_dpd_303P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T']\n",
      "####### NAN count = 514070\n",
      "####### NAN count = 606920\n",
      "####### NAN count = 263233\n",
      "####### NAN count = 517511\n",
      "####### NAN count = 545885\n",
      "####### NAN count = 636453\n",
      "####### NAN count = 512650\n",
      "Use these ['max_overdueamount_659A', 'max_numberofoverdueinstls_725L']\n",
      "####### NAN count = 263171\n",
      "Use these ['max_overdueamountmax2_14A', 'max_totaloutstanddebtvalue_39A', 'max_dateofcredend_289D', 'max_dateofcredstart_739D', 'max_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'max_pmts_month_158T', 'max_pmts_year_1139T']\n",
      "####### NAN count = 262653\n",
      "Use these ['max_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_numberofoverdueinstlmax_1151L']\n",
      "####### NAN count = 512590\n",
      "Use these ['max_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T']\n",
      "####### NAN count = 513987\n",
      "####### NAN count = 1039597\n",
      "####### NAN count = 606900\n",
      "####### NAN count = 545855\n",
      "####### NAN count = 636448\n",
      "Use these ['max_totaldebtoverduevalue_718A', 'max_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L']\n",
      "####### NAN count = 297072\n",
      "####### NAN count = 512961\n",
      "####### NAN count = 512591\n",
      "####### NAN count = 802351\n",
      "####### NAN count = 1012361\n",
      "####### NAN count = 806653\n",
      "####### NAN count = 1007594\n",
      "####### NAN count = 822517\n",
      "####### NAN count = 745109\n",
      "####### NAN count = 545898\n",
      "####### NAN count = 636545\n",
      "####### NAN count = 545895\n",
      "####### NAN count = 636544\n",
      "####### NAN count = 512657\n",
      "####### NAN count = 561307\n",
      "####### NAN count = 649082\n",
      "####### NAN count = 140386\n",
      "Use these ['max_contractdate_551D', 'max_pmts_date_1107D']\n",
      "####### NAN count = 1490212\n",
      "####### NAN count = 1490235\n",
      "####### NAN count = 1514201\n",
      "####### NAN count = 959958\n",
      "####### NAN count = 1467040\n",
      "####### NAN count = 1421548\n",
      "####### NAN count = 1421572\n",
      "####### NAN count = 262659\n",
      "####### NAN count = 512884\n",
      "Use these ['max_pmts_month_706T', 'max_pmts_year_507T']\n",
      "####### NAN count = 512598\n",
      "Use these ['max_num_group1_13', 'max_num_group2_13']\n",
      "####### NAN count = 141371\n",
      "####### NAN count = 1520903\n",
      "Use these ['max_num_group1_15', 'max_num_group2_15']\n",
      "####### NAN count = 91554\n",
      "['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9', 'assignmentdate_238D', 'assignmentdate_4527235D', 'assignmentdate_4955616D', 'birthdate_574D', 'dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'dateofbirth_342D', 'pmtscount_423L', 'pmtssum_45A', 'responsedate_1012D', 'responsedate_4527233D', 'responsedate_4917613D', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'mindbdtollast24m_4525191P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'datefirstoffer_1144D', 'datelastinstal40dpd_247D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'max_num_group1', 'max_num_group2_14', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'lastrepayingdate_696D', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'payvacationpostpone_4187118D', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'validfrom_1069D', 'max_actualdpd_943P', 'max_annuity_853A', 'max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'max_currdebt_94A', 'max_mainoccupationinc_437A', 'max_maxdpdtolerance_577P', 'max_outstandingdebt_522A', 'max_approvaldate_319D', 'max_dateactivated_425D', 'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'max_recorddate_4527225D', 'max_deductiondate_4917603D', 'max_pmtamount_36A', 'max_processingdate_168D', 'max_num_group1_5', 'max_credlmt_230A', 'max_credlmt_935A', 'max_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'max_pmts_dpd_303P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'max_instlamount_768A', 'max_monthlyinstlamount_332A', 'max_monthlyinstlamount_674A', 'max_outstandingamount_354A', 'max_outstandingamount_362A', 'max_overdueamount_31A', 'max_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'max_overdueamountmax2_14A', 'max_totaloutstanddebtvalue_39A', 'max_dateofcredend_289D', 'max_dateofcredstart_739D', 'max_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'max_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_numberofoverdueinstlmax_1151L', 'max_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'max_residualamount_488A', 'max_residualamount_856A', 'max_totalamount_6A', 'max_totalamount_996A', 'max_totaldebtoverduevalue_718A', 'max_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_dateofrealrepmt_138D', 'max_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'max_numberofoverdueinstlmaxdat_641D', 'max_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'max_num_group1_6', 'max_contractdate_551D', 'max_pmts_date_1107D', 'max_contractmaturitydate_151D', 'max_birthdate_87D', 'max_empl_employedfrom_271D', 'max_contractenddate_991D', 'max_openingdate_313D', 'max_openingdate_857D', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'max_empls_employedfrom_796D', 'max_num_group1_15', 'max_num_group2_15']\n",
      "241\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "df = feature_eng(**data_store) # import train data \n",
    "print(\"train data shape:\\t\", df.shape)\n",
    "# gc.collect()\n",
    "# spamming gc.collect praying for memory to not full\n",
    "gc.collect()\n",
    "df = df.pipe(Pipeline.filter_cols) # fillter column\n",
    "gc.collect()\n",
    "df, cat_cols = to_pandas(df) # tranform to pandas dataframe, easier to work with\n",
    "gc.collect()\n",
    "df = reduce_mem_usage(df) # as the name said\n",
    "gc.collect()\n",
    "print(\"train data shape:\\t\", df.shape)\n",
    "nums=df.select_dtypes(exclude='category').columns\n",
    "# IDK what is going on for now\n",
    "from itertools import combinations, permutations\n",
    "#df=df[nums]\n",
    "nans_df = df[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "            #print(str(gg)+'-'+str(n),', ',end='')\n",
    "        use.append(vx)\n",
    "        #print()\n",
    "    print('Use these',use)\n",
    "    return use\n",
    "\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    correlation_matrix = matrix.corr()\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            #cross_features=list(combinations(Vs, 2))\n",
    "            #make_corr(Vs)\n",
    "            grps= group_columns_by_correlation(df[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "            #make_corr(use)\n",
    "    else:\n",
    "        uses=uses+v\n",
    "    print('####### NAN count =',k)\n",
    "print(uses)\n",
    "print(len(uses))\n",
    "uses=uses+list(df.select_dtypes(include='category').columns)\n",
    "print(len(uses))\n",
    "df=df[uses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fe797",
   "metadata": {
    "papermill": {
     "duration": 0.006268,
     "end_time": "2024-02-07T21:28:21.080353",
     "exception": false,
     "start_time": "2024-02-07T21:28:21.074085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fca359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b52589",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 16:14:07,056 - /home/sabina.jangirova/.conda/envs/ai701/lib/python3.8/site-packages/castle/backend/__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2024-05-10 16:14:07,086 - /home/sabina.jangirova/.conda/envs/ai701/lib/python3.8/site-packages/castle/algorithms/__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import dowhy.gcm as gcm\n",
    "from castle.algorithms import PC\n",
    "from castle.algorithms.pc.pc import find_skeleton\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74998a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(input_graph, node_lookup):\n",
    "    '''\n",
    "    Function to visualise graphs.\n",
    "\n",
    "    Args:\n",
    "        input_graph (array): Adjacency matrix representing graph\n",
    "        node_lookup (dict): Dictionary containing node names.\n",
    "    '''\n",
    "    \n",
    "    graph = nx.DiGraph(input_graph)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    nx.draw(\n",
    "    G=graph,\n",
    "    node_color=COLORS[0],\n",
    "    node_size=8000,\n",
    "    arrowsize=17,\n",
    "    with_labels=True,\n",
    "    labels=node_lookup,\n",
    "    font_color='white',\n",
    "    font_size=9,\n",
    "    pos=nx.circular_layout(graph)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfff514f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "df = df.drop(columns=[\"case_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe5ab67",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df[cat_cols] = df[cat_cols].astype(str)\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Fit Ordinal Encoder on Training Data\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
    "encoder.fit(df[cat_cols])\n",
    "\n",
    "# Transform Training Data\n",
    "df[cat_cols] = encoder.transform(df[cat_cols])\n",
    "df[cat_cols] = df[cat_cols].fillna(-1)\n",
    "df[cat_cols] = df[cat_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee1c0c2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6002083b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "node_lookup = {i: column for i, column in enumerate(df.columns)}\n",
    "total_nodes = len(node_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f4a327",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1f3ef0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3d9dad",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "weeks = X_train['WEEK_NUM']\n",
    "X_train = X_train.drop(columns=['WEEK_NUM'])\n",
    "X_test = X_test.drop(columns=['WEEK_NUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41fb33d6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b14bc9c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(X_train)\n",
    "n_samples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74bdb9d9",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import time\n",
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "import resource\n",
    "\n",
    "def using():\n",
    "    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    return usage[2]/1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d24b5fc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "edg_list = load('saved_models/fci/edges.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f17f65a3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "X6 <-> X251\n",
      "X84 <-> X251\n",
      "X168 <-> X251\n",
      "X196 <-> X251\n",
      "X5 <-> X6\n",
      "X5 <-> X176\n",
      "X6 <-> X81\n",
      "X6 <-> X151\n",
      "X6 <-> X251\n",
      "X7 <-> X46\n",
      "X7 <-> X226\n",
      "X8 <-> X164\n",
      "X10 <-> X164\n",
      "X15 <-> X16\n",
      "X16 <-> X17\n",
      "X16 <-> X18\n",
      "X16 <-> X20\n",
      "X16 <-> X204\n",
      "X18 <-> X169\n",
      "X20 <-> X156\n",
      "X21 <-> X84\n",
      "X21 <-> X186\n",
      "X23 <-> X26\n",
      "X24 <-> X26\n",
      "X26 <-> X39\n",
      "X26 <-> X101\n",
      "X26 <-> X115\n",
      "X26 <-> X143\n",
      "X27 <-> X116\n",
      "X29 <-> X96\n",
      "X29 <-> X164\n",
      "X30 <-> X36\n",
      "X30 <-> X106\n",
      "X31 <-> X36\n",
      "X32 <-> X36\n",
      "X32 <-> X69\n",
      "X32 <-> X236\n",
      "X34 <-> X36\n",
      "X34 <-> X165\n",
      "X36 <-> X112\n",
      "X36 <-> X135\n",
      "X36 <-> X146\n",
      "X36 <-> X152\n",
      "X38 <-> X167\n",
      "X38 <-> X206\n",
      "X41 <-> X136\n",
      "X44 <-> X46\n",
      "X44 <-> X136\n",
      "X46 <-> X48\n",
      "X46 <-> X51\n",
      "X46 <-> X58\n",
      "X46 <-> X178\n",
      "X46 <-> X249\n",
      "X48 <-> X56\n",
      "X50 <-> X56\n",
      "X51 <-> X56\n",
      "X52 <-> X65\n",
      "X53 <-> X56\n",
      "X54 <-> X61\n",
      "X55 <-> X69\n",
      "X56 <-> X57\n",
      "X56 <-> X61\n",
      "X56 <-> X63\n",
      "X58 <-> X60\n",
      "X58 <-> X184\n",
      "X59 <-> X63\n",
      "X65 --> X59\n",
      "X59 <-> X67\n",
      "X60 <-> X64\n",
      "X60 <-> X72\n",
      "X60 <-> X87\n",
      "X60 <-> X105\n",
      "X60 <-> X247\n",
      "X61 <-> X62\n",
      "X61 <-> X64\n",
      "X61 <-> X91\n",
      "X61 <-> X131\n",
      "X61 <-> X207\n",
      "X62 <-> X63\n",
      "X62 <-> X73\n",
      "X62 <-> X74\n",
      "X62 <-> X75\n",
      "X63 <-> X65\n",
      "X63 <-> X88\n",
      "X63 <-> X99\n",
      "X63 <-> X128\n",
      "X64 <-> X66\n",
      "X64 <-> X176\n",
      "X64 <-> X196\n",
      "X65 <-> X66\n",
      "X66 <-> X67\n",
      "X66 <-> X68\n",
      "X67 <-> X77\n",
      "X67 <-> X87\n",
      "X67 <-> X107\n",
      "X68 <-> X71\n",
      "X68 <-> X82\n",
      "X68 <-> X83\n",
      "X68 <-> X87\n",
      "X68 <-> X88\n",
      "X68 <-> X96\n",
      "X68 <-> X150\n",
      "X69 <-> X84\n",
      "X69 <-> X92\n",
      "X69 <-> X249\n",
      "X71 <-> X166\n",
      "X74 <-> X76\n",
      "X74 <-> X161\n",
      "X74 <-> X206\n",
      "X75 <-> X160\n",
      "X76 <-> X77\n",
      "X76 <-> X78\n",
      "X76 <-> X82\n",
      "X76 <-> X83\n",
      "X76 <-> X94\n",
      "X76 <-> X110\n",
      "X76 <-> X119\n",
      "X76 <-> X130\n",
      "X76 <-> X155\n",
      "X77 <-> X84\n",
      "X77 <-> X96\n",
      "X77 <-> X116\n",
      "X79 <-> X163\n",
      "X79 <-> X246\n",
      "X80 <-> X84\n",
      "X82 <-> X84\n",
      "X82 <-> X167\n",
      "X83 <-> X84\n",
      "X84 <-> X85\n",
      "X84 <-> X87\n",
      "X84 <-> X251\n",
      "X85 <-> X96\n",
      "X86 <-> X87\n",
      "X86 <-> X88\n",
      "X86 <-> X93\n",
      "X86 <-> X100\n",
      "X86 <-> X160\n",
      "X86 <-> X169\n",
      "X88 <-> X186\n",
      "X91 <-> X116\n",
      "X92 <-> X184\n",
      "X92 <-> X246\n",
      "X93 <-> X96\n",
      "X94 <-> X169\n",
      "X96 <-> X97\n",
      "X96 <-> X182\n",
      "X99 <-> X106\n",
      "X99 <-> X186\n",
      "X99 <-> X196\n",
      "X100 <-> X146\n",
      "X103 <-> X116\n",
      "X105 <-> X106\n",
      "X105 <-> X176\n",
      "X106 <-> X142\n",
      "X106 <-> X182\n",
      "X109 <-> X146\n",
      "X109 <-> X206\n",
      "X109 <-> X246\n",
      "X111 <-> X216\n",
      "X113 <-> X184\n",
      "X116 <-> X168\n",
      "X116 <-> X209\n",
      "X118 <-> X216\n",
      "X119 <-> X168\n",
      "X123 <-> X146\n",
      "X123 <-> X196\n",
      "X124 <-> X166\n",
      "X126 <-> X176\n",
      "X126 <-> X185\n",
      "X126 <-> X198\n",
      "X126 <-> X201\n",
      "X126 <-> X227\n",
      "X126 <-> X238\n",
      "X130 <-> X167\n",
      "X130 <-> X216\n",
      "X131 <-> X164\n",
      "X132 <-> X136\n",
      "X132 <-> X176\n",
      "X134 <-> X156\n",
      "X135 <-> X136\n",
      "X135 <-> X156\n",
      "X138 <-> X146\n",
      "X139 <-> X166\n",
      "X142 <-> X196\n",
      "X146 <-> X148\n",
      "X146 <-> X149\n",
      "X146 <-> X171\n",
      "X146 <-> X250\n",
      "X147 <-> X246\n",
      "X149 <-> X236\n",
      "X150 <-> X216\n",
      "X153 <-> X169\n",
      "X154 <-> X156\n",
      "X162 --> X154\n",
      "X155 <-> X246\n",
      "X158 <-> X160\n",
      "X159 <-> X165\n",
      "X160 <-> X193\n",
      "X161 <-> X162\n",
      "X161 <-> X176\n",
      "X162 <-> X163\n",
      "X163 <-> X176\n",
      "X164 <-> X180\n",
      "X164 <-> X192\n",
      "X164 <-> X198\n",
      "X164 <-> X201\n",
      "X165 <-> X196\n",
      "X165 <-> X206\n",
      "X165 <-> X214\n",
      "X165 <-> X217\n",
      "X165 <-> X220\n",
      "X165 <-> X247\n",
      "X166 <-> X212\n",
      "X166 <-> X227\n",
      "X166 <-> X240\n",
      "X166 <-> X249\n",
      "X167 <-> X173\n",
      "X167 <-> X194\n",
      "X167 <-> X198\n",
      "X167 <-> X201\n",
      "X168 <-> X189\n",
      "X168 <-> X229\n",
      "X168 <-> X251\n",
      "X169 <-> X179\n",
      "X169 <-> X184\n",
      "X175 <-> X176\n",
      "X176 <-> X177\n",
      "X176 <-> X178\n",
      "X176 <-> X180\n",
      "X176 <-> X220\n",
      "X181 <-> X184\n",
      "X182 <-> X184\n",
      "X184 <-> X207\n",
      "X186 <-> X190\n",
      "X186 <-> X205\n",
      "X196 <-> X235\n",
      "X196 <-> X251\n",
      "X199 <-> X206\n",
      "X202 <-> X236\n",
      "X204 <-> X206\n",
      "X206 <-> X208\n",
      "X206 <-> X229\n",
      "X210 <-> X246\n",
      "X215 <-> X216\n",
      "X217 <-> X236\n",
      "X221 <-> X246\n",
      "X223 <-> X226\n",
      "X226 <-> X241\n",
      "X226 <-> X244\n",
      "X226 <-> X245\n",
      "X233 <-> X236\n",
      "--------\n",
      "X129 <-> X251\n",
      "X186 <-> X251\n",
      "X238 <-> X251\n",
      "X4 <-> X129\n",
      "X27 <-> X238\n",
      "X51 <-> X129\n",
      "X55 <-> X129\n",
      "X94 <-> X129\n",
      "X109 <-> X238\n",
      "X118 <-> X186\n",
      "X124 <-> X129\n",
      "X129 <-> X251\n",
      "X139 <-> X186\n",
      "X186 <-> X195\n",
      "X186 <-> X230\n",
      "X186 <-> X251\n",
      "X218 <-> X238\n",
      "X237 <-> X238\n",
      "X238 <-> X251\n",
      "--------\n",
      "X86 <-> X251\n",
      "X200 <-> X251\n",
      "X209 <-> X251\n",
      "X9 <-> X186\n",
      "X40 <-> X86\n",
      "X58 <-> X86\n",
      "X86 <-> X88\n",
      "X86 <-> X93\n",
      "X86 <-> X229\n",
      "X86 <-> X251\n",
      "X94 <-> X209\n",
      "X116 <-> X200\n",
      "X186 <-> X187\n",
      "X186 <-> X237\n",
      "X197 o-> X209\n",
      "X200 <-> X251\n",
      "X209 <-> X251\n",
      "--------\n",
      "X36 <-> X251\n",
      "X96 <-> X251\n",
      "X179 <-> X251\n",
      "X2 <-> X136\n",
      "X24 <-> X179\n",
      "X36 <-> X158\n",
      "X36 <-> X209\n",
      "X36 <-> X251\n",
      "X40 o-> X179\n",
      "X53 <-> X196\n",
      "X85 <-> X196\n",
      "X96 <-> X241\n",
      "X96 <-> X243\n",
      "X96 <-> X251\n",
      "X134 <-> X179\n",
      "X135 <-> X136\n",
      "X136 <-> X137\n",
      "X136 <-> X139\n",
      "X160 <-> X196\n",
      "X164 <-> X179\n",
      "X166 <-> X236\n",
      "X179 <-> X246\n",
      "X179 <-> X251\n",
      "X196 <-> X222\n",
      "X233 <-> X236\n",
      "--------\n",
      "X251 o-> X109\n",
      "X251 o-> X109\n"
     ]
    }
   ],
   "source": [
    "components = []\n",
    "for edges in edg_list:\n",
    "    components.append([])\n",
    "    print(\"--------\")\n",
    "    for e in edges:\n",
    "        if \"X251\" in str(e):\n",
    "            print(str(e))\n",
    "            components[-1].append(str(e).split(\" \")[0][1:])\n",
    "    generation_back = []\n",
    "    for e in edges:\n",
    "        if any(node in str(e) for node in components[-1]):\n",
    "            print(str(e))\n",
    "            generation_back.append(str(e).split(\" \")[0][1:])\n",
    "    components[-1] = components[-1] + generation_back\n",
    "    components[-1] = list(set(components[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c1ae365",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# fci_times = []\n",
    "# fci_memory = []\n",
    "# graphs = []\n",
    "# edg_list = []\n",
    "# for n in range(n_samples, 99, -100):\n",
    "#     random_indeces = np.random.choice(X_train.shape[0], size=n, replace=False)\n",
    "#     X_train_part = features_standardized[random_indeces]\n",
    "#     y_train = np.array(y_train)\n",
    "#     y_train_part = y_train[random_indeces]\n",
    "#     y_train_part = y_train_part.reshape((n, 1))\n",
    "#     graph_data = np.concatenate((X_train_part, y_train_part), axis=1)\n",
    "#     start_time = time.time()\n",
    "#     start_memory = using()\n",
    "#     g, edges = fci(graph_data, independence_test_method=\"fisherz\")\n",
    "#     fci_memory.append(start_memory - using())\n",
    "#     fci_times.append(start_time - time.time())\n",
    "#     graphs.append(g)\n",
    "#     edg_list.append(edges)\n",
    "#     pdy = GraphUtils.to_pydot(g)\n",
    "#     pdy.write_png(f'all_features_graph_{n}_samples.png')\n",
    "#     gc.collect()\n",
    "#     print(f\"{n} samples finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a3f2699",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device_type\": \"cpu\", \n",
    "    \"verbose\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1be1a08",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ipca = load(\"saved_models/pca/250_n_components_1_fold_828348114496282_auc.joblib\")\n",
    "features_standardized = ipca.transform(features_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3e9c29c5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:['18', '160', '175', '85', '93', '123', '153', '24', '94', '46', '32', '63', '48', '223', '154', '124', '164', '88', '130', '161', '21', '92', '83', '68', '54', '16', '86', '217', '118', '169', '75', '7', '119', '65', '233', '27', '84', '29', '64', '41', '80', '226', '146', '158', '51', '126', '210', '38', '34', '36', '53', '69', '202', '163', '149', '56', '147', '8', '138', '159', '60', '155', '166', '26', '74', '199', '184', '44', '131', '66', '91', '15', '215', '109', '116', '71', '135', '50', '31', '206', '82', '134', '132', '6', '52', '113', '111', '20', '204', '100', '150', '106', '182', '76', '99', '167', '10', '142', '196', '61', '77', '62', '176', '186', '59', '55', '5', '221', '30', '96', '105', '139', '162', '23', '67', '79', '165', '58', '181', '103', '168']\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30855\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.783212\n",
      "[400]\tvalid_0's auc: 0.792347\n",
      "[600]\tvalid_0's auc: 0.796072\n",
      "[800]\tvalid_0's auc: 0.797811\n",
      "[1000]\tvalid_0's auc: 0.798857\n",
      "[1200]\tvalid_0's auc: 0.799558\n",
      "[1400]\tvalid_0's auc: 0.800053\n",
      "[1600]\tvalid_0's auc: 0.800626\n",
      "[1800]\tvalid_0's auc: 0.800686\n",
      "Early stopping, best iteration is:\n",
      "[1818]\tvalid_0's auc: 0.800707\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30855\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.789492\n",
      "[400]\tvalid_0's auc: 0.800068\n",
      "[600]\tvalid_0's auc: 0.804106\n",
      "[800]\tvalid_0's auc: 0.805898\n",
      "[1000]\tvalid_0's auc: 0.807096\n",
      "[1200]\tvalid_0's auc: 0.807834\n",
      "[1400]\tvalid_0's auc: 0.808213\n",
      "Early stopping, best iteration is:\n",
      "[1463]\tvalid_0's auc: 0.808369\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30855\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.783446\n",
      "[400]\tvalid_0's auc: 0.793489\n",
      "[600]\tvalid_0's auc: 0.797509\n",
      "[800]\tvalid_0's auc: 0.799067\n",
      "[1000]\tvalid_0's auc: 0.800521\n",
      "[1200]\tvalid_0's auc: 0.801454\n",
      "[1400]\tvalid_0's auc: 0.801793\n",
      "Early stopping, best iteration is:\n",
      "[1451]\tvalid_0's auc: 0.801845\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30855\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.788692\n",
      "[400]\tvalid_0's auc: 0.798966\n",
      "[600]\tvalid_0's auc: 0.802484\n",
      "[800]\tvalid_0's auc: 0.804228\n",
      "[1000]\tvalid_0's auc: 0.805247\n",
      "[1200]\tvalid_0's auc: 0.805923\n",
      "[1400]\tvalid_0's auc: 0.806457\n",
      "[1600]\tvalid_0's auc: 0.80702\n",
      "[1800]\tvalid_0's auc: 0.806946\n",
      "Early stopping, best iteration is:\n",
      "[1712]\tvalid_0's auc: 0.807173\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 30855\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 121\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.777391\n",
      "[400]\tvalid_0's auc: 0.786629\n",
      "[600]\tvalid_0's auc: 0.790094\n",
      "[800]\tvalid_0's auc: 0.791848\n",
      "[1000]\tvalid_0's auc: 0.792701\n",
      "[1200]\tvalid_0's auc: 0.793416\n",
      "[1400]\tvalid_0's auc: 0.793909\n",
      "[1600]\tvalid_0's auc: 0.794117\n",
      "[1800]\tvalid_0's auc: 0.794393\n",
      "[2000]\tvalid_0's auc: 0.794347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1927]\tvalid_0's auc: 0.794463\n",
      "Components:['186', '94', '55', '237', '124', '238', '27', '129', '118', '51', '4', '109', '218', '139']\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.685868\n",
      "[400]\tvalid_0's auc: 0.690589\n",
      "[600]\tvalid_0's auc: 0.692642\n",
      "[800]\tvalid_0's auc: 0.693454\n",
      "[1000]\tvalid_0's auc: 0.693809\n",
      "[1200]\tvalid_0's auc: 0.6939\n",
      "Early stopping, best iteration is:\n",
      "[1256]\tvalid_0's auc: 0.694061\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.688207\n",
      "[400]\tvalid_0's auc: 0.692796\n",
      "[600]\tvalid_0's auc: 0.694119\n",
      "Early stopping, best iteration is:\n",
      "[694]\tvalid_0's auc: 0.694554\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.685508\n",
      "[400]\tvalid_0's auc: 0.690266\n",
      "[600]\tvalid_0's auc: 0.692023\n",
      "[800]\tvalid_0's auc: 0.692745\n",
      "Early stopping, best iteration is:\n",
      "[824]\tvalid_0's auc: 0.692814\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.69771\n",
      "[400]\tvalid_0's auc: 0.701824\n",
      "[600]\tvalid_0's auc: 0.703324\n",
      "[800]\tvalid_0's auc: 0.703735\n",
      "Early stopping, best iteration is:\n",
      "[737]\tvalid_0's auc: 0.703771\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.683596\n",
      "[400]\tvalid_0's auc: 0.687338\n",
      "[600]\tvalid_0's auc: 0.688677\n",
      "[800]\tvalid_0's auc: 0.689306\n",
      "Early stopping, best iteration is:\n",
      "[754]\tvalid_0's auc: 0.689415\n",
      "Components:['40', '86', '209', '186', '94', '197', '9', '58', '200', '116']\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.620884\n",
      "[400]\tvalid_0's auc: 0.62331\n",
      "[600]\tvalid_0's auc: 0.623597\n",
      "Early stopping, best iteration is:\n",
      "[539]\tvalid_0's auc: 0.623696\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.623192\n",
      "[400]\tvalid_0's auc: 0.627607\n",
      "[600]\tvalid_0's auc: 0.628377\n",
      "Early stopping, best iteration is:\n",
      "[566]\tvalid_0's auc: 0.628498\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.621462\n",
      "[400]\tvalid_0's auc: 0.625205\n",
      "[600]\tvalid_0's auc: 0.626515\n",
      "[800]\tvalid_0's auc: 0.626507\n",
      "Early stopping, best iteration is:\n",
      "[730]\tvalid_0's auc: 0.626912\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.623013\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's auc: 0.625086\n",
      "[600]\tvalid_0's auc: 0.626326\n",
      "Early stopping, best iteration is:\n",
      "[517]\tvalid_0's auc: 0.626371\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's auc: 0.611925\n",
      "[400]\tvalid_0's auc: 0.615707\n",
      "[600]\tvalid_0's auc: 0.616412\n",
      "[800]\tvalid_0's auc: 0.616532\n",
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's auc: 0.616661\n",
      "Components:['40', '24', '135', '233', '196', '36', '179', '53', '166', '96', '2', '164', '160', '134', '136', '85']\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.659882\n",
      "[400]\tvalid_0's auc: 0.664393\n",
      "[600]\tvalid_0's auc: 0.665689\n",
      "[800]\tvalid_0's auc: 0.665882\n",
      "[1000]\tvalid_0's auc: 0.666116\n",
      "Early stopping, best iteration is:\n",
      "[1009]\tvalid_0's auc: 0.66624\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004387 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.664825\n",
      "[400]\tvalid_0's auc: 0.671908\n",
      "[600]\tvalid_0's auc: 0.674456\n",
      "[800]\tvalid_0's auc: 0.675509\n",
      "[1000]\tvalid_0's auc: 0.67623\n",
      "[1200]\tvalid_0's auc: 0.676693\n",
      "[1400]\tvalid_0's auc: 0.677388\n",
      "[1600]\tvalid_0's auc: 0.677671\n",
      "Early stopping, best iteration is:\n",
      "[1574]\tvalid_0's auc: 0.677829\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.663752\n",
      "[400]\tvalid_0's auc: 0.66963\n",
      "[600]\tvalid_0's auc: 0.672101\n",
      "[800]\tvalid_0's auc: 0.67304\n",
      "[1000]\tvalid_0's auc: 0.673835\n",
      "[1200]\tvalid_0's auc: 0.674112\n",
      "Early stopping, best iteration is:\n",
      "[1215]\tvalid_0's auc: 0.674193\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.663849\n",
      "[400]\tvalid_0's auc: 0.669738\n",
      "[600]\tvalid_0's auc: 0.671789\n",
      "[800]\tvalid_0's auc: 0.672591\n",
      "[1000]\tvalid_0's auc: 0.673365\n",
      "[1200]\tvalid_0's auc: 0.673605\n",
      "Early stopping, best iteration is:\n",
      "[1265]\tvalid_0's auc: 0.673901\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.659398\n",
      "[400]\tvalid_0's auc: 0.663853\n",
      "[600]\tvalid_0's auc: 0.665626\n",
      "[800]\tvalid_0's auc: 0.66668\n",
      "[1000]\tvalid_0's auc: 0.667048\n",
      "[1200]\tvalid_0's auc: 0.667317\n",
      "Early stopping, best iteration is:\n",
      "[1126]\tvalid_0's auc: 0.667431\n"
     ]
    }
   ],
   "source": [
    "lbgm_fci_times = []\n",
    "lgbm_fci_memory = []\n",
    "cv_scores = []\n",
    "fitted_models = []\n",
    "\n",
    "for c in components[:4]:\n",
    "    print(f\"Components:{c}\")\n",
    "    cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "    f = 0\n",
    "    c = [int(k) for k in c]\n",
    "    for idx_train, idx_valid in cv.split(features_standardized, y_train, groups=weeks):\n",
    "        print(f\"Fold #{f}\")\n",
    "        X_train_l, y_train_l = features_standardized[idx_train], y_train.iloc[idx_train]\n",
    "        X_valid, y_valid = features_standardized[idx_valid], y_train.iloc[idx_valid]\n",
    "        X_train_reduced = X_train_l[:, c]\n",
    "        X_valid_reduced = X_valid[:, c]\n",
    "        start_time = time.time()\n",
    "        start_memory = using()\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_reduced, y_train_l,\n",
    "            eval_set = [(X_valid_reduced, y_valid)],\n",
    "            callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "        fitted_models.append(model)\n",
    "        lbgm_fci_times.append(start_time - time.time())\n",
    "        lgbm_fci_memory.append(start_memory - using())\n",
    "        y_pred_valid = model.predict_proba(X_valid_reduced)[:,1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "        cv_scores.append(auc_score)\n",
    "        f+=1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f19c4da3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC scores:  [0.6167224175172841, 0.6192343737176871, 0.6166453354394873, 0.6211125255989607, 0.6108099811852672, 0.5763609808523109, 0.5748252119741343, 0.574092686111206, 0.5795192684021264, 0.5719988401350442, 0.5644166277963943, 0.5612872889201967, 0.5609980424884227, 0.5613329702578147, 0.555736446406293, 0.5572091016232772, 0.5494137299745139, 0.5569395651023228, 0.5560817263373599, 0.5487591766063173]\n",
      "Maximum CV AUC score:  0.6211125255989607\n"
     ]
    }
   ],
   "source": [
    "print(\"CV AUC scores: \", cv_scores)\n",
    "print(\"Maximum CV AUC score: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "312e1a59",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), [6, 84, 168, 196])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), [6, 84, 168, 196])' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m components[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m components[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Predict labels for validation data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m fitted_models[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n\u001b[1;32m     14\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred_labels)\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/core/indexes/base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3628\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3631\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ai701/lib/python3.8/site-packages/pandas/core/indexes/base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5635\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5636\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), [6, 84, 168, 196])"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# import numpy as np\n",
    "\n",
    "# conf_matrix = None\n",
    "# class_report = None\n",
    "\n",
    "# components[0] = [int(k) for k in components[0]]\n",
    "\n",
    "# # Predict labels for validation data\n",
    "# y_pred_labels = fitted_models[3].predict(X_test[:, components[0]])\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# # Compute classification report\n",
    "# class_report = classification_report(y_test, y_pred_labels)\n",
    "\n",
    "# # Print confusion matrix and classification report\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f332bf7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/fci/1_more_generation_fitted_models.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(lbgm_fci_times, \"./saved_models/fci/1_more_generation_lbgm_fci_times.joblib\")\n",
    "dump(lgbm_fci_memory, \"./saved_models/fci/1_more_generation_lgbm_fci_memory.joblib\")\n",
    "dump(cv_scores, \"./saved_models/fci/1_more_generation_cv_scores.joblib\")\n",
    "dump(fitted_models, \"./saved_models/fci/1_more_generation_fitted_models.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3491f34",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# import numpy as np\n",
    "\n",
    "# conf_matrix = None\n",
    "# class_report = None\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_test = ipcas[1].transform(X_test)\n",
    "\n",
    "# # Predict labels for validation data\n",
    "# y_pred_labels = fitted_models[1].predict(X_test)\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# # Compute classification report\n",
    "# class_report = classification_report(y_test, y_pred_labels)\n",
    "\n",
    "# # Print confusion matrix and classification report\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8541beab",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# n_components = 250\n",
    "# counter = 0\n",
    "# for auc, ipca in zip(cv_scores, ipcas):\n",
    "#     if counter == 5:\n",
    "#         counter = 0\n",
    "#         n_components -= 50\n",
    "#     auc = str(auc)\n",
    "#     auc = auc.split('.')[1]\n",
    "#     filename = f\"{n_components}_n_components_{counter}_fold_{auc}_auc.joblib\"\n",
    "#     dump(ipca, \"./saved_models/pca/\"+filename)\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8a98f13",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ipca = IncrementalPCA(n_components=300)\n",
    "# principalComponents = ipca.fit_transform(features_standardized)\n",
    "# principalDf = pd.DataFrame(data = principalComponents,\n",
    "#                            columns = ['PC' + str(i) for i in range(1, ipca.n_components_ + 1)])\n",
    "# finalDf = pd.concat([principalDf, df_train[['target']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "863bb5ea",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(\"Original number of features:\", features.shape[1])\n",
    "# print(\"Reduced number of features:\", principalDf.shape[1])\n",
    "\n",
    "# # Display some of the data\n",
    "# print(finalDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df5fbf1f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from causallearn.search.ConstraintBased.PC import PC\n",
    "# from causallearn.utils.GraphUtils import GraphUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdb5caeb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# data = df_train_part.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6ebeb1a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# g, edges = fci(data, independence_test_method=\"chisq\", verbose=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da26fd95",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pdy = GraphUtils.to_pydot(g)\n",
    "# pdy.write_png('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2352caae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pc = PC(variant=\"stable\")\n",
    "# pc.learn(df_train_part)\n",
    "# graph_pred = pc.causal_matrix\n",
    "\n",
    "# graph_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7375f261",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# y = df_train[\"target\"]\n",
    "# weeks = df_train[\"WEEK_NUM\"]\n",
    "# df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "# cv = StratifiedGroupKFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eed7be3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:21.095200Z",
     "iopub.status.busy": "2024-02-07T21:28:21.094816Z",
     "iopub.status.idle": "2024-02-07T21:28:22.748078Z",
     "shell.execute_reply": "2024-02-07T21:28:22.747011Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 1.664029,
     "end_time": "2024-02-07T21:28:22.750906",
     "exception": false,
     "start_time": "2024-02-07T21:28:21.086877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train[cat_cols] = df_train[cat_cols].astype(str)\n",
    "# import polars as pl\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# # Fit Ordinal Encoder on Training Data\n",
    "# encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
    "# encoder.fit(df_train[cat_cols])\n",
    "\n",
    "# # Transform Training Data\n",
    "# df_train[cat_cols] = encoder.transform(df_train[cat_cols])\n",
    "# df_train[cat_cols] = df_train[cat_cols].fillna(-1)\n",
    "# df_train[cat_cols] = df_train[cat_cols].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3cc4a29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:22.766849Z",
     "iopub.status.busy": "2024-02-07T21:28:22.766164Z",
     "iopub.status.idle": "2024-02-07T21:28:22.780619Z",
     "shell.execute_reply": "2024-02-07T21:28:22.779457Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.025248,
     "end_time": "2024-02-07T21:28:22.783159",
     "exception": false,
     "start_time": "2024-02-07T21:28:22.757911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
    "#     pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
    "#     (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    "# )\n",
    "\n",
    "# test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "#     pl.col(\"num_group1\") == 0\n",
    "# ).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "# test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "#     pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "#     (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
    "# )\n",
    "\n",
    "# data_submission = test_basetable.join(\n",
    "#     test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7195e82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:22.798812Z",
     "iopub.status.busy": "2024-02-07T21:28:22.798415Z",
     "iopub.status.idle": "2024-02-07T21:28:32.218372Z",
     "shell.execute_reply": "2024-02-07T21:28:32.217254Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 9.431294,
     "end_time": "2024-02-07T21:28:32.221469",
     "exception": false,
     "start_time": "2024-02-07T21:28:22.790175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "# case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
    "# case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "# cols_pred = []\n",
    "# for col in data.columns:\n",
    "#     if col[-1].isupper() and col[:-1].islower():\n",
    "#         cols_pred.append(col)\n",
    "\n",
    "# print(cols_pred)\n",
    "\n",
    "# def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "#     return (\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
    "#     )\n",
    "\n",
    "# base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "# base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "# base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
    "\n",
    "# for df in [X_train, X_valid, X_test]:\n",
    "#     df = convert_strings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a275858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:32.238895Z",
     "iopub.status.busy": "2024-02-07T21:28:32.238439Z",
     "iopub.status.idle": "2024-02-07T21:28:32.246480Z",
     "shell.execute_reply": "2024-02-07T21:28:32.244638Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.019901,
     "end_time": "2024-02-07T21:28:32.249064",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.229163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Train: {X_train.shape}\")\n",
    "# print(f\"Valid: {X_valid.shape}\")\n",
    "# print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f3324",
   "metadata": {
    "papermill": {
     "duration": 0.006806,
     "end_time": "2024-02-07T21:28:32.263196",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.256390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training LightGBM\n",
    "\n",
    "Minimal example of LightGBM training is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb1d2bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:32.278983Z",
     "iopub.status.busy": "2024-02-07T21:28:32.278570Z",
     "iopub.status.idle": "2024-02-07T21:29:52.839799Z",
     "shell.execute_reply": "2024-02-07T21:29:52.838506Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 80.572251,
     "end_time": "2024-02-07T21:29:52.842351",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.270100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "# lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
    "\n",
    "# params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"metric\": \"auc\",\n",
    "#     \"max_depth\": 3,\n",
    "#     \"num_leaves\": 31,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"feature_fraction\": 0.9,\n",
    "#     \"bagging_fraction\": 0.8,\n",
    "#     \"bagging_freq\": 5,\n",
    "#     \"n_estimators\": 1000,\n",
    "#     \"verbose\": -1,\n",
    "# }\n",
    "\n",
    "# gbm = lgb.train(\n",
    "#     params,\n",
    "#     lgb_train,\n",
    "#     valid_sets=lgb_valid,\n",
    "#     callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e73d6",
   "metadata": {
    "papermill": {
     "duration": 0.008222,
     "end_time": "2024-02-07T21:29:52.859056",
     "exception": false,
     "start_time": "2024-02-07T21:29:52.850834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluation with AUC and then comparison with the stability metric is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e5c4fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:29:52.878460Z",
     "iopub.status.busy": "2024-02-07T21:29:52.877756Z",
     "iopub.status.idle": "2024-02-07T21:30:15.482110Z",
     "shell.execute_reply": "2024-02-07T21:30:15.480907Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 22.617002,
     "end_time": "2024-02-07T21:30:15.484653",
     "exception": false,
     "start_time": "2024-02-07T21:29:52.867651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "#     y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n",
    "#     base[\"score\"] = y_pred\n",
    "\n",
    "# print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \n",
    "# print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "# print(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e10914f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:30:15.504198Z",
     "iopub.status.busy": "2024-02-07T21:30:15.503723Z",
     "iopub.status.idle": "2024-02-07T21:30:16.621500Z",
     "shell.execute_reply": "2024-02-07T21:30:16.620020Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 1.131134,
     "end_time": "2024-02-07T21:30:16.624526",
     "exception": false,
     "start_time": "2024-02-07T21:30:15.493392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "#     gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "#         .sort_values(\"WEEK_NUM\")\\\n",
    "#         .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "#         .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "#     x = np.arange(len(gini_in_time))\n",
    "#     y = gini_in_time\n",
    "#     a, b = np.polyfit(x, y, 1)\n",
    "#     y_hat = a*x + b\n",
    "#     residuals = y - y_hat\n",
    "#     res_std = np.std(residuals)\n",
    "#     avg_gini = np.mean(gini_in_time)\n",
    "#     return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "# stability_score_train = gini_stability(base_train)\n",
    "# stability_score_valid = gini_stability(base_valid)\n",
    "# stability_score_test = gini_stability(base_test)\n",
    "\n",
    "# print(f'The stability score on the train set is: {stability_score_train}') \n",
    "# print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "# print(f'The stability score on the test set is: {stability_score_test}')  "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7493015,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 145.018038,
   "end_time": "2024-02-07T21:30:18.166484",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-07T21:27:53.148446",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00187238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:27:57.520412Z",
     "iopub.status.busy": "2024-02-07T21:27:57.520013Z",
     "iopub.status.idle": "2024-02-07T21:28:01.736666Z",
     "shell.execute_reply": "2024-02-07T21:28:01.735326Z"
    },
    "papermill": {
     "duration": 4.227484,
     "end_time": "2024-02-07T21:28:01.739621",
     "exception": false,
     "start_time": "2024-02-07T21:27:57.512137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ROOT = Path('/home/sabina.jangirova/Documents/ML703_project/data')\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b1f90b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:01.755445Z",
     "iopub.status.busy": "2024-02-07T21:28:01.755022Z",
     "iopub.status.idle": "2024-02-07T21:28:01.764738Z",
     "shell.execute_reply": "2024-02-07T21:28:01.763428Z"
    },
    "papermill": {
     "duration": 0.020544,
     "end_time": "2024-02-07T21:28:01.767089",
     "exception": false,
     "start_time": "2024-02-07T21:28:01.746545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date,strict=False))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns: \n",
    "                if col.endswith(\"D\"):\n",
    "                    # Calculate the difference in days between each date column and date_decision\n",
    "                    df = df.with_columns(\n",
    "                        (pl.col(\"date_decision\") - pl.col(col)).dt.total_days().alias(col)\n",
    "                    )\n",
    "                    df = df.with_columns(pl.col(col).fill_null(np.nan)) \n",
    "        # Drop date_decision column\n",
    "        df = df.drop(\"date_decision\")\n",
    "#         print(df.dtypes) # for Debugging\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df,base_df = None,test=False):\n",
    "        #for test data\n",
    "            for col in df.columns:\n",
    "                if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                    isnull = df[col].is_null().mean()\n",
    "                    if isnull > 0.7:\n",
    "                        df = df.drop(col)\n",
    "            columns_to_drop = []\n",
    "            for col in df.columns:\n",
    "                if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                    freq = df[col].n_unique()\n",
    "                    if (freq == 1) or (freq > 100):\n",
    "                        columns_to_drop.append(col)\n",
    "\n",
    "            df = df.drop(columns_to_drop)\n",
    "            return df\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs\n",
    "\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.set_table_dtypes)\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                try:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "                except:\n",
    "                    continue\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016b91f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:01.782683Z",
     "iopub.status.busy": "2024-02-07T21:28:01.782161Z",
     "iopub.status.idle": "2024-02-07T21:28:20.978046Z",
     "shell.execute_reply": "2024-02-07T21:28:20.977092Z"
    },
    "papermill": {
     "duration": 19.207089,
     "end_time": "2024-02-07T21:28:20.980847",
     "exception": false,
     "start_time": "2024-02-07T21:28:01.773758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8d8336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:20.996615Z",
     "iopub.status.busy": "2024-02-07T21:28:20.995823Z",
     "iopub.status.idle": "2024-02-07T21:28:21.064631Z",
     "shell.execute_reply": "2024-02-07T21:28:21.063597Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.079736,
     "end_time": "2024-02-07T21:28:21.067361",
     "exception": false,
     "start_time": "2024-02-07T21:28:20.987625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 2min 26s, total: 6min 35s\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n",
    "        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da15e81",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 488)\n",
      "Memory usage of dataframe is 3313.73 MB\n",
      "Memory usage after optimization is: 1097.80 MB\n",
      "Decreased by 66.9%\n",
      "train data shape:\t (1526659, 344)\n",
      "Use these ['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9']\n",
      "####### NAN count = 0\n",
      "####### NAN count = 1389663\n",
      "####### NAN count = 1411681\n",
      "####### NAN count = 1455026\n",
      "####### NAN count = 918788\n",
      "Use these ['dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D']\n",
      "####### NAN count = 140968\n",
      "####### NAN count = 1490159\n",
      "Use these ['pmtscount_423L', 'pmtssum_45A']\n",
      "####### NAN count = 954021\n",
      "####### NAN count = 806659\n",
      "####### NAN count = 866332\n",
      "####### NAN count = 1301747\n",
      "####### NAN count = 418178\n",
      "Use these ['amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L']\n",
      "####### NAN count = 561124\n",
      "Use these ['annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A']\n",
      "####### NAN count = 4\n",
      "Use these ['mindbddpdlast24m_3658935P']\n",
      "####### NAN count = 613202\n",
      "####### NAN count = 948244\n",
      "Use these ['mindbdtollast24m_4525191P']\n",
      "####### NAN count = 972827\n",
      "####### NAN count = 467175\n",
      "Use these ['avginstallast24m_3658937A', 'maxinstallast24m_3658928A']\n",
      "####### NAN count = 624875\n",
      "####### NAN count = 757006\n",
      "####### NAN count = 841181\n",
      "####### NAN count = 1026987\n",
      "####### NAN count = 455190\n",
      "####### NAN count = 460822\n",
      "Use these ['commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P']\n",
      "####### NAN count = 343375\n",
      "####### NAN count = 833735\n",
      "####### NAN count = 1392841\n",
      "####### NAN count = 887659\n",
      "Use these ['daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L']\n",
      "####### NAN count = 452594\n",
      "####### NAN count = 977119\n",
      "Use these ['eir_270L']\n",
      "####### NAN count = 190833\n",
      "####### NAN count = 859214\n",
      "####### NAN count = 482103\n",
      "####### NAN count = 453587\n",
      "Use these ['lastapplicationdate_877D', 'max_num_group1', 'max_num_group2_14']\n",
      "####### NAN count = 305137\n",
      "Use these ['lastapprcredamount_781A', 'lastapprdate_640D']\n",
      "####### NAN count = 442041\n",
      "####### NAN count = 977975\n",
      "Use these ['lastrejectcredamount_222A', 'lastrejectdate_50D']\n",
      "####### NAN count = 769046\n",
      "####### NAN count = 1524282\n",
      "####### NAN count = 511255\n",
      "Use these ['mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P']\n",
      "####### NAN count = 306019\n",
      "####### NAN count = 960953\n",
      "####### NAN count = 705504\n",
      "####### NAN count = 876276\n",
      "####### NAN count = 826000\n",
      "####### NAN count = 829402\n",
      "####### NAN count = 1032856\n",
      "####### NAN count = 766958\n",
      "Use these ['numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L']\n",
      "####### NAN count = 452593\n",
      "####### NAN count = 455081\n",
      "Use these ['numinstlsallpaid_934L']\n",
      "####### NAN count = 445669\n",
      "Use these ['numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L']\n",
      "####### NAN count = 456495\n",
      "Use these ['numinstpaid_4499208L']\n",
      "####### NAN count = 847191\n",
      "####### NAN count = 446983\n",
      "Use these ['numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A']\n",
      "####### NAN count = 840646\n",
      "####### NAN count = 669186\n",
      "####### NAN count = 455612\n",
      "####### NAN count = 1517330\n",
      "Use these ['pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L']\n",
      "####### NAN count = 458738\n",
      "####### NAN count = 461362\n",
      "####### NAN count = 459827\n",
      "####### NAN count = 460079\n",
      "####### NAN count = 44954\n",
      "####### NAN count = 78526\n",
      "####### NAN count = 131888\n",
      "####### NAN count = 181122\n",
      "####### NAN count = 223240\n",
      "####### NAN count = 445320\n",
      "####### NAN count = 3\n",
      "####### NAN count = 1374886\n",
      "####### NAN count = 305154\n",
      "####### NAN count = 308739\n",
      "Use these ['max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A']\n",
      "####### NAN count = 307441\n",
      "####### NAN count = 419006\n",
      "####### NAN count = 306361\n",
      "####### NAN count = 450969\n",
      "####### NAN count = 420383\n",
      "####### NAN count = 442999\n",
      "####### NAN count = 454678\n",
      "####### NAN count = 703840\n",
      "####### NAN count = 548987\n",
      "####### NAN count = 559169\n",
      "####### NAN count = 334873\n",
      "####### NAN count = 961606\n",
      "####### NAN count = 552766\n",
      "Use these ['max_pmtnum_8L']\n",
      "####### NAN count = 321446\n",
      "####### NAN count = 1068725\n",
      "####### NAN count = 1375927\n",
      "Use these ['max_pmtamount_36A', 'max_processingdate_168D', 'max_num_group1_5']\n",
      "####### NAN count = 1044394\n",
      "####### NAN count = 1036944\n",
      "####### NAN count = 603001\n",
      "Use these ['max_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T']\n",
      "####### NAN count = 263166\n",
      "Use these ['max_pmts_dpd_303P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T']\n",
      "####### NAN count = 514070\n",
      "####### NAN count = 606920\n",
      "####### NAN count = 263233\n",
      "####### NAN count = 517511\n",
      "####### NAN count = 545885\n",
      "####### NAN count = 636453\n",
      "####### NAN count = 512650\n",
      "Use these ['max_overdueamount_659A', 'max_numberofoverdueinstls_725L']\n",
      "####### NAN count = 263171\n",
      "Use these ['max_overdueamountmax2_14A', 'max_totaloutstanddebtvalue_39A', 'max_dateofcredend_289D', 'max_dateofcredstart_739D', 'max_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'max_pmts_month_158T', 'max_pmts_year_1139T']\n",
      "####### NAN count = 262653\n",
      "Use these ['max_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_numberofoverdueinstlmax_1151L']\n",
      "####### NAN count = 512590\n",
      "Use these ['max_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T']\n",
      "####### NAN count = 513987\n",
      "####### NAN count = 1039597\n",
      "####### NAN count = 606900\n",
      "####### NAN count = 545855\n",
      "####### NAN count = 636448\n",
      "Use these ['max_totaldebtoverduevalue_718A', 'max_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L']\n",
      "####### NAN count = 297072\n",
      "####### NAN count = 512961\n",
      "####### NAN count = 512591\n",
      "####### NAN count = 802351\n",
      "####### NAN count = 1012361\n",
      "####### NAN count = 806653\n",
      "####### NAN count = 1007594\n",
      "####### NAN count = 822517\n",
      "####### NAN count = 745109\n",
      "####### NAN count = 545898\n",
      "####### NAN count = 636545\n",
      "####### NAN count = 545895\n",
      "####### NAN count = 636544\n",
      "####### NAN count = 512657\n",
      "####### NAN count = 561307\n",
      "####### NAN count = 649082\n",
      "####### NAN count = 140386\n",
      "Use these ['max_contractdate_551D', 'max_pmts_date_1107D']\n",
      "####### NAN count = 1490212\n",
      "####### NAN count = 1490235\n",
      "####### NAN count = 1514201\n",
      "####### NAN count = 959958\n",
      "####### NAN count = 1467040\n",
      "####### NAN count = 1421548\n",
      "####### NAN count = 1421572\n",
      "####### NAN count = 262659\n",
      "####### NAN count = 512884\n",
      "Use these ['max_pmts_month_706T', 'max_pmts_year_507T']\n",
      "####### NAN count = 512598\n",
      "Use these ['max_num_group1_13', 'max_num_group2_13']\n",
      "####### NAN count = 141371\n",
      "####### NAN count = 1520903\n",
      "Use these ['max_num_group1_15', 'max_num_group2_15']\n",
      "####### NAN count = 91554\n",
      "['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9', 'assignmentdate_238D', 'assignmentdate_4527235D', 'assignmentdate_4955616D', 'birthdate_574D', 'dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'dateofbirth_342D', 'pmtscount_423L', 'pmtssum_45A', 'responsedate_1012D', 'responsedate_4527233D', 'responsedate_4917613D', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'mindbdtollast24m_4525191P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'datefirstoffer_1144D', 'datelastinstal40dpd_247D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'max_num_group1', 'max_num_group2_14', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'lastrepayingdate_696D', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'payvacationpostpone_4187118D', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'validfrom_1069D', 'max_actualdpd_943P', 'max_annuity_853A', 'max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'max_currdebt_94A', 'max_mainoccupationinc_437A', 'max_maxdpdtolerance_577P', 'max_outstandingdebt_522A', 'max_approvaldate_319D', 'max_dateactivated_425D', 'max_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'max_recorddate_4527225D', 'max_deductiondate_4917603D', 'max_pmtamount_36A', 'max_processingdate_168D', 'max_num_group1_5', 'max_credlmt_230A', 'max_credlmt_935A', 'max_dpdmax_139P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'max_pmts_dpd_303P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'max_instlamount_768A', 'max_monthlyinstlamount_332A', 'max_monthlyinstlamount_674A', 'max_outstandingamount_354A', 'max_outstandingamount_362A', 'max_overdueamount_31A', 'max_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'max_overdueamountmax2_14A', 'max_totaloutstanddebtvalue_39A', 'max_dateofcredend_289D', 'max_dateofcredstart_739D', 'max_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'max_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'max_numberofoverdueinstlmax_1151L', 'max_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'max_residualamount_488A', 'max_residualamount_856A', 'max_totalamount_6A', 'max_totalamount_996A', 'max_totaldebtoverduevalue_718A', 'max_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_dateofrealrepmt_138D', 'max_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'max_numberofoverdueinstlmaxdat_641D', 'max_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'max_num_group1_6', 'max_contractdate_551D', 'max_pmts_date_1107D', 'max_contractmaturitydate_151D', 'max_birthdate_87D', 'max_empl_employedfrom_271D', 'max_contractenddate_991D', 'max_openingdate_313D', 'max_openingdate_857D', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'max_empls_employedfrom_796D', 'max_num_group1_15', 'max_num_group2_15']\n",
      "241\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "df = feature_eng(**data_store) # import train data \n",
    "print(\"train data shape:\\t\", df.shape)\n",
    "# gc.collect()\n",
    "# spamming gc.collect praying for memory to not full\n",
    "gc.collect()\n",
    "df = df.pipe(Pipeline.filter_cols) # fillter column\n",
    "gc.collect()\n",
    "df, cat_cols = to_pandas(df) # tranform to pandas dataframe, easier to work with\n",
    "gc.collect()\n",
    "df = reduce_mem_usage(df) # as the name said\n",
    "gc.collect()\n",
    "print(\"train data shape:\\t\", df.shape)\n",
    "nums=df.select_dtypes(exclude='category').columns\n",
    "# IDK what is going on for now\n",
    "from itertools import combinations, permutations\n",
    "#df=df[nums]\n",
    "nans_df = df[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n",
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "            #print(str(gg)+'-'+str(n),', ',end='')\n",
    "        use.append(vx)\n",
    "        #print()\n",
    "    print('Use these',use)\n",
    "    return use\n",
    "\n",
    "def group_columns_by_correlation(matrix, threshold=0.8):\n",
    "    correlation_matrix = matrix.corr()\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    return groups\n",
    "\n",
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            #cross_features=list(combinations(Vs, 2))\n",
    "            #make_corr(Vs)\n",
    "            grps= group_columns_by_correlation(df[Vs], threshold=0.8)\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "            #make_corr(use)\n",
    "    else:\n",
    "        uses=uses+v\n",
    "    print('####### NAN count =',k)\n",
    "print(uses)\n",
    "print(len(uses))\n",
    "uses=uses+list(df.select_dtypes(include='category').columns)\n",
    "print(len(uses))\n",
    "df=df[uses]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fe797",
   "metadata": {
    "papermill": {
     "duration": 0.006268,
     "end_time": "2024-02-07T21:28:21.080353",
     "exception": false,
     "start_time": "2024-02-07T21:28:21.074085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fca359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b52589",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 12:40:23,639 - /home/sabina.jangirova/.conda/envs/ai701/lib/python3.8/site-packages/castle/backend/__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2024-05-10 12:40:23,779 - /home/sabina.jangirova/.conda/envs/ai701/lib/python3.8/site-packages/castle/algorithms/__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import dowhy.gcm as gcm\n",
    "from castle.algorithms import PC\n",
    "from castle.algorithms.pc.pc import find_skeleton\n",
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74998a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(input_graph, node_lookup):\n",
    "    '''\n",
    "    Function to visualise graphs.\n",
    "\n",
    "    Args:\n",
    "        input_graph (array): Adjacency matrix representing graph\n",
    "        node_lookup (dict): Dictionary containing node names.\n",
    "    '''\n",
    "    \n",
    "    graph = nx.DiGraph(input_graph)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    nx.draw(\n",
    "    G=graph,\n",
    "    node_color=COLORS[0],\n",
    "    node_size=8000,\n",
    "    arrowsize=17,\n",
    "    with_labels=True,\n",
    "    labels=node_lookup,\n",
    "    font_color='white',\n",
    "    font_size=9,\n",
    "    pos=nx.circular_layout(graph)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfff514f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y = df['target']\n",
    "df = df.drop(columns=[\"case_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbe5ab67",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df[cat_cols] = df[cat_cols].astype(str)\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Fit Ordinal Encoder on Training Data\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
    "encoder.fit(df[cat_cols])\n",
    "\n",
    "# Transform Training Data\n",
    "df[cat_cols] = encoder.transform(df[cat_cols])\n",
    "df[cat_cols] = df[cat_cols].fillna(-1)\n",
    "df[cat_cols] = df[cat_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee1c0c2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6002083b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "node_lookup = {i: column for i, column in enumerate(df.columns)}\n",
    "total_nodes = len(node_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f4a327",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c1f3ef0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3d9dad",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "weeks = X_train['WEEK_NUM']\n",
    "X_train = X_train.drop(columns=['WEEK_NUM'])\n",
    "X_test = X_test.drop(columns=['WEEK_NUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41fb33d6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b14bc9c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a3f2699",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device_type\": \"cpu\", \n",
    "    \"verbose\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0245ebc8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import resource\n",
    "\n",
    "def using():\n",
    "    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    return usage[2]/1024.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e9c29c5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 250\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 250\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.806676\n",
      "[400]\tvalid_0's auc: 0.813711\n",
      "[600]\tvalid_0's auc: 0.816517\n",
      "[800]\tvalid_0's auc: 0.817372\n",
      "[1000]\tvalid_0's auc: 0.817821\n",
      "[1200]\tvalid_0's auc: 0.818144\n",
      "Early stopping, best iteration is:\n",
      "[1143]\tvalid_0's auc: 0.818259\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 250\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.814106\n",
      "[400]\tvalid_0's auc: 0.822594\n",
      "[600]\tvalid_0's auc: 0.825906\n",
      "[800]\tvalid_0's auc: 0.827235\n",
      "[1000]\tvalid_0's auc: 0.827876\n",
      "[1200]\tvalid_0's auc: 0.82823\n",
      "[1400]\tvalid_0's auc: 0.828287\n",
      "Early stopping, best iteration is:\n",
      "[1320]\tvalid_0's auc: 0.828348\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 250\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.804791\n",
      "[400]\tvalid_0's auc: 0.812531\n",
      "[600]\tvalid_0's auc: 0.815513\n",
      "[800]\tvalid_0's auc: 0.817187\n",
      "[1000]\tvalid_0's auc: 0.818275\n",
      "[1200]\tvalid_0's auc: 0.818772\n",
      "[1400]\tvalid_0's auc: 0.818982\n",
      "Early stopping, best iteration is:\n",
      "[1390]\tvalid_0's auc: 0.819047\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63750\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 250\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.812155\n",
      "[400]\tvalid_0's auc: 0.820239\n",
      "[600]\tvalid_0's auc: 0.823252\n",
      "[800]\tvalid_0's auc: 0.824394\n",
      "[1000]\tvalid_0's auc: 0.825009\n",
      "[1200]\tvalid_0's auc: 0.82574\n",
      "[1400]\tvalid_0's auc: 0.82587\n",
      "[1600]\tvalid_0's auc: 0.826161\n",
      "Early stopping, best iteration is:\n",
      "[1698]\tvalid_0's auc: 0.826185\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 63750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 250\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.80075\n",
      "[400]\tvalid_0's auc: 0.809309\n",
      "[600]\tvalid_0's auc: 0.812324\n",
      "[800]\tvalid_0's auc: 0.813558\n",
      "[1000]\tvalid_0's auc: 0.814133\n",
      "[1200]\tvalid_0's auc: 0.814408\n",
      "[1400]\tvalid_0's auc: 0.814599\n",
      "Early stopping, best iteration is:\n",
      "[1358]\tvalid_0's auc: 0.814687\n",
      "Number of components: 200\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.803288\n",
      "[400]\tvalid_0's auc: 0.81036\n",
      "[600]\tvalid_0's auc: 0.81292\n",
      "[800]\tvalid_0's auc: 0.814019\n",
      "[1000]\tvalid_0's auc: 0.814376\n",
      "Early stopping, best iteration is:\n",
      "[919]\tvalid_0's auc: 0.814401\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.809956\n",
      "[400]\tvalid_0's auc: 0.818488\n",
      "[600]\tvalid_0's auc: 0.821753\n",
      "[800]\tvalid_0's auc: 0.823036\n",
      "[1000]\tvalid_0's auc: 0.823898\n",
      "[1200]\tvalid_0's auc: 0.824671\n",
      "[1400]\tvalid_0's auc: 0.825135\n",
      "Early stopping, best iteration is:\n",
      "[1480]\tvalid_0's auc: 0.8252\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.802751\n",
      "[400]\tvalid_0's auc: 0.810015\n",
      "[600]\tvalid_0's auc: 0.813165\n",
      "[800]\tvalid_0's auc: 0.814415\n",
      "[1000]\tvalid_0's auc: 0.815145\n",
      "[1200]\tvalid_0's auc: 0.815508\n",
      "[1400]\tvalid_0's auc: 0.815655\n",
      "[1600]\tvalid_0's auc: 0.816074\n",
      "Early stopping, best iteration is:\n",
      "[1698]\tvalid_0's auc: 0.816265\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.810061\n",
      "[400]\tvalid_0's auc: 0.817643\n",
      "[600]\tvalid_0's auc: 0.820427\n",
      "[800]\tvalid_0's auc: 0.821649\n",
      "[1000]\tvalid_0's auc: 0.822455\n",
      "[1200]\tvalid_0's auc: 0.822724\n",
      "[1400]\tvalid_0's auc: 0.823058\n",
      "[1600]\tvalid_0's auc: 0.823177\n",
      "Early stopping, best iteration is:\n",
      "[1570]\tvalid_0's auc: 0.823225\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 51000\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.795743\n",
      "[400]\tvalid_0's auc: 0.804025\n",
      "[600]\tvalid_0's auc: 0.807601\n",
      "[800]\tvalid_0's auc: 0.809039\n",
      "[1000]\tvalid_0's auc: 0.809533\n",
      "[1200]\tvalid_0's auc: 0.810211\n",
      "Early stopping, best iteration is:\n",
      "[1248]\tvalid_0's auc: 0.810339\n",
      "Number of components: 150\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38250\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.80107\n",
      "[400]\tvalid_0's auc: 0.807637\n",
      "[600]\tvalid_0's auc: 0.810141\n",
      "[800]\tvalid_0's auc: 0.811278\n",
      "[1000]\tvalid_0's auc: 0.812061\n",
      "[1200]\tvalid_0's auc: 0.812346\n",
      "[1400]\tvalid_0's auc: 0.812602\n",
      "Early stopping, best iteration is:\n",
      "[1427]\tvalid_0's auc: 0.812653\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38250\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.806735\n",
      "[400]\tvalid_0's auc: 0.814147\n",
      "[600]\tvalid_0's auc: 0.817247\n",
      "[800]\tvalid_0's auc: 0.818728\n",
      "[1000]\tvalid_0's auc: 0.819384\n",
      "[1200]\tvalid_0's auc: 0.819944\n",
      "[1400]\tvalid_0's auc: 0.820323\n",
      "[1600]\tvalid_0's auc: 0.82035\n",
      "Early stopping, best iteration is:\n",
      "[1560]\tvalid_0's auc: 0.820439\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38250\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.798244\n",
      "[400]\tvalid_0's auc: 0.805027\n",
      "[600]\tvalid_0's auc: 0.807788\n",
      "[800]\tvalid_0's auc: 0.808863\n",
      "[1000]\tvalid_0's auc: 0.809327\n",
      "[1200]\tvalid_0's auc: 0.809453\n",
      "Early stopping, best iteration is:\n",
      "[1119]\tvalid_0's auc: 0.809648\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111418 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38250\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.807571\n",
      "[400]\tvalid_0's auc: 0.814687\n",
      "[600]\tvalid_0's auc: 0.81705\n",
      "[800]\tvalid_0's auc: 0.818265\n",
      "[1000]\tvalid_0's auc: 0.818865\n",
      "[1200]\tvalid_0's auc: 0.819271\n",
      "[1400]\tvalid_0's auc: 0.819255\n",
      "[1600]\tvalid_0's auc: 0.8194\n",
      "Early stopping, best iteration is:\n",
      "[1541]\tvalid_0's auc: 0.819489\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 38250\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 150\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.794518\n",
      "[400]\tvalid_0's auc: 0.802419\n",
      "[600]\tvalid_0's auc: 0.805747\n",
      "[800]\tvalid_0's auc: 0.807322\n",
      "[1000]\tvalid_0's auc: 0.808193\n",
      "[1200]\tvalid_0's auc: 0.808722\n",
      "[1400]\tvalid_0's auc: 0.80893\n",
      "[1600]\tvalid_0's auc: 0.809326\n",
      "Early stopping, best iteration is:\n",
      "[1610]\tvalid_0's auc: 0.809402\n",
      "Number of components: 100\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.798823\n",
      "[400]\tvalid_0's auc: 0.804876\n",
      "[600]\tvalid_0's auc: 0.807586\n",
      "[800]\tvalid_0's auc: 0.808694\n",
      "[1000]\tvalid_0's auc: 0.809168\n",
      "[1200]\tvalid_0's auc: 0.809488\n",
      "[1400]\tvalid_0's auc: 0.809494\n",
      "Early stopping, best iteration is:\n",
      "[1333]\tvalid_0's auc: 0.809585\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.804501\n",
      "[400]\tvalid_0's auc: 0.811641\n",
      "[600]\tvalid_0's auc: 0.813997\n",
      "[800]\tvalid_0's auc: 0.815065\n",
      "[1000]\tvalid_0's auc: 0.815632\n",
      "[1200]\tvalid_0's auc: 0.815894\n",
      "Early stopping, best iteration is:\n",
      "[1299]\tvalid_0's auc: 0.816137\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.79648\n",
      "[400]\tvalid_0's auc: 0.802771\n",
      "[600]\tvalid_0's auc: 0.80527\n",
      "[800]\tvalid_0's auc: 0.806473\n",
      "[1000]\tvalid_0's auc: 0.807121\n",
      "[1200]\tvalid_0's auc: 0.80746\n",
      "[1400]\tvalid_0's auc: 0.807755\n",
      "[1600]\tvalid_0's auc: 0.807892\n",
      "Early stopping, best iteration is:\n",
      "[1598]\tvalid_0's auc: 0.807922\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.803196\n",
      "[400]\tvalid_0's auc: 0.809538\n",
      "[600]\tvalid_0's auc: 0.811785\n",
      "[800]\tvalid_0's auc: 0.813245\n",
      "[1000]\tvalid_0's auc: 0.814031\n",
      "[1200]\tvalid_0's auc: 0.814369\n",
      "[1400]\tvalid_0's auc: 0.814463\n",
      "Early stopping, best iteration is:\n",
      "[1344]\tvalid_0's auc: 0.814571\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25500\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.793112\n",
      "[400]\tvalid_0's auc: 0.800078\n",
      "[600]\tvalid_0's auc: 0.802518\n",
      "[800]\tvalid_0's auc: 0.803608\n",
      "[1000]\tvalid_0's auc: 0.804266\n",
      "[1200]\tvalid_0's auc: 0.804673\n",
      "[1400]\tvalid_0's auc: 0.804941\n",
      "Early stopping, best iteration is:\n",
      "[1407]\tvalid_0's auc: 0.80497\n",
      "Number of components: 50\n",
      "Fold #0\n",
      "[LightGBM] [Info] Number of positive: 30178, number of negative: 947050\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977228, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030881 -> initscore=-3.446239\n",
      "[LightGBM] [Info] Start training from score -3.446239\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.789818\n",
      "[400]\tvalid_0's auc: 0.795148\n",
      "[600]\tvalid_0's auc: 0.797065\n",
      "[800]\tvalid_0's auc: 0.79799\n",
      "[1000]\tvalid_0's auc: 0.798167\n",
      "[1200]\tvalid_0's auc: 0.798461\n",
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's auc: 0.798478\n",
      "Fold #1\n",
      "[LightGBM] [Info] Number of positive: 30985, number of negative: 946298\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977283, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031705 -> initscore=-3.419054\n",
      "[LightGBM] [Info] Start training from score -3.419054\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.794416\n",
      "[400]\tvalid_0's auc: 0.800834\n",
      "[600]\tvalid_0's auc: 0.803261\n",
      "[800]\tvalid_0's auc: 0.804258\n",
      "[1000]\tvalid_0's auc: 0.804924\n",
      "[1200]\tvalid_0's auc: 0.805168\n",
      "[1400]\tvalid_0's auc: 0.805503\n",
      "[1600]\tvalid_0's auc: 0.805714\n",
      "Early stopping, best iteration is:\n",
      "[1534]\tvalid_0's auc: 0.805758\n",
      "Fold #2\n",
      "[LightGBM] [Info] Number of positive: 31087, number of negative: 946197\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977284, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031810 -> initscore=-3.415661\n",
      "[LightGBM] [Info] Start training from score -3.415661\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.784357\n",
      "[400]\tvalid_0's auc: 0.790324\n",
      "[600]\tvalid_0's auc: 0.792667\n",
      "[800]\tvalid_0's auc: 0.793661\n",
      "[1000]\tvalid_0's auc: 0.793995\n",
      "[1200]\tvalid_0's auc: 0.794645\n",
      "[1400]\tvalid_0's auc: 0.794859\n",
      "[1600]\tvalid_0's auc: 0.795062\n",
      "Early stopping, best iteration is:\n",
      "[1555]\tvalid_0's auc: 0.79516\n",
      "Fold #3\n",
      "[LightGBM] [Info] Number of positive: 30855, number of negative: 945600\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010841 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 976455, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031599 -> initscore=-3.422521\n",
      "[LightGBM] [Info] Start training from score -3.422521\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.793237\n",
      "[400]\tvalid_0's auc: 0.799461\n",
      "[600]\tvalid_0's auc: 0.801852\n",
      "[800]\tvalid_0's auc: 0.803045\n",
      "[1000]\tvalid_0's auc: 0.803763\n",
      "[1200]\tvalid_0's auc: 0.804152\n",
      "[1400]\tvalid_0's auc: 0.804498\n",
      "[1600]\tvalid_0's auc: 0.804763\n",
      "[1800]\tvalid_0's auc: 0.804879\n",
      "Early stopping, best iteration is:\n",
      "[1765]\tvalid_0's auc: 0.804952\n",
      "Fold #4\n",
      "[LightGBM] [Info] Number of positive: 30659, number of negative: 946399\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12750\n",
      "[LightGBM] [Info] Number of data points in the train set: 977058, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031379 -> initscore=-3.429738\n",
      "[LightGBM] [Info] Start training from score -3.429738\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's auc: 0.78168\n",
      "[400]\tvalid_0's auc: 0.788197\n",
      "[600]\tvalid_0's auc: 0.790692\n",
      "[800]\tvalid_0's auc: 0.79203\n",
      "[1000]\tvalid_0's auc: 0.792859\n",
      "[1200]\tvalid_0's auc: 0.793468\n",
      "[1400]\tvalid_0's auc: 0.793688\n",
      "[1600]\tvalid_0's auc: 0.794049\n",
      "[1800]\tvalid_0's auc: 0.794246\n",
      "Early stopping, best iteration is:\n",
      "[1837]\tvalid_0's auc: 0.794305\n"
     ]
    }
   ],
   "source": [
    "fitted_models = []\n",
    "cv_scores = []\n",
    "ipcas = []\n",
    "pca_times = []\n",
    "pca_memory = []\n",
    "lgbm_pca_times = []\n",
    "lgbm_pcs_memory = []\n",
    "\n",
    "for n in range(250, 49, -50):\n",
    "    print(f\"Number of components: {n}\")\n",
    "    cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "    f = 0\n",
    "    for idx_train, idx_valid in cv.split(features_standardized, y_train, groups=weeks):\n",
    "        print(f\"Fold #{f}\")\n",
    "        X_train_l, y_train_l = features_standardized[idx_train], y_train.iloc[idx_train]\n",
    "        X_valid, y_valid = features_standardized[idx_valid], y_train.iloc[idx_valid]\n",
    "        start_time = time.time()\n",
    "        start_memory = using()\n",
    "        ipca = IncrementalPCA(n_components=n, batch_size=256)\n",
    "        ipca.fit(X_train_l)\n",
    "        ipcas.append(ipca)\n",
    "        X_train_reduced = ipca.transform(X_train_l)\n",
    "        X_valid_reduced = ipca.transform(X_valid)\n",
    "        pca_times.append(start_time - time.time())\n",
    "        pca_memory.append(start_memory - using())\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_memory = using()\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train_reduced, y_train_l,\n",
    "            eval_set = [(X_valid_reduced, y_valid)],\n",
    "            callbacks = [lgb.log_evaluation(200), lgb.early_stopping(100)] )\n",
    "        fitted_models.append(model)\n",
    "        lgbm_pca_times.append(start_time - time.time())\n",
    "        lgbm_pcs_memory.append(start_memory - using())\n",
    "        \n",
    "        y_pred_valid = model.predict_proba(X_valid_reduced)[:,1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred_valid)\n",
    "        cv_scores.append(auc_score)\n",
    "        f+=1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f332bf7",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./saved_models/pca/lgbm_pcs_memory.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(fitted_models, \"./saved_models/pca/fitted_models.joblib\")\n",
    "dump(cv_scores, \"./saved_models/pca/cv_scores.joblib\")\n",
    "dump(ipcas, \"./saved_models/pca/ipcas.joblib\")\n",
    "dump(pca_times, \"./saved_models/pca/pca_times.joblib\")\n",
    "dump(pca_memory, \"./saved_models/pca/pca_memory.joblib\")\n",
    "dump(lgbm_pca_times, \"./saved_models/pca/lgbm_pca_times.joblib\")\n",
    "dump(lgbm_pcs_memory, \"./saved_models/pca/lgbm_pcs_memory.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19c4da3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(\"CV AUC scores: \", cv_scores)\n",
    "# print(\"Maximum CV AUC score: \", max(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3491f34",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# import numpy as np\n",
    "\n",
    "# conf_matrix = None\n",
    "# class_report = None\n",
    "# X_test = scaler.transform(X_test)\n",
    "# X_test = ipcas[1].transform(X_test)\n",
    "\n",
    "# # Predict labels for validation data\n",
    "# y_pred_labels = fitted_models[1].predict(X_test)\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# # Compute classification report\n",
    "# class_report = classification_report(y_test, y_pred_labels)\n",
    "\n",
    "# # Print confusion matrix and classification report\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8541beab",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# n_components = 250\n",
    "# counter = 0\n",
    "# for auc, ipca in zip(cv_scores, ipcas):\n",
    "#     if counter == 5:\n",
    "#         counter = 0\n",
    "#         n_components -= 50\n",
    "#     auc = str(auc)\n",
    "#     auc = auc.split('.')[1]\n",
    "#     filename = f\"{n_components}_n_components_{counter}_fold_{auc}_auc.joblib\"\n",
    "#     dump(ipca, \"./saved_models/pca/\"+filename)\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a98f13",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ipca = IncrementalPCA(n_components=300)\n",
    "# principalComponents = ipca.fit_transform(features_standardized)\n",
    "# principalDf = pd.DataFrame(data = principalComponents,\n",
    "#                            columns = ['PC' + str(i) for i in range(1, ipca.n_components_ + 1)])\n",
    "# finalDf = pd.concat([principalDf, df_train[['target']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863bb5ea",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(\"Original number of features:\", features.shape[1])\n",
    "# print(\"Reduced number of features:\", principalDf.shape[1])\n",
    "\n",
    "# # Display some of the data\n",
    "# print(finalDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df5fbf1f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# from causallearn.search.ConstraintBased.PC import PC\n",
    "# from causallearn.utils.GraphUtils import GraphUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdb5caeb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# data = df_train_part.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ebeb1a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# g, edges = fci(data, independence_test_method=\"chisq\", verbose=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da26fd95",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pdy = GraphUtils.to_pydot(g)\n",
    "# pdy.write_png('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2352caae",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pc = PC(variant=\"stable\")\n",
    "# pc.learn(df_train_part)\n",
    "# graph_pred = pc.causal_matrix\n",
    "\n",
    "# graph_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7375f261",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# y = df_train[\"target\"]\n",
    "# weeks = df_train[\"WEEK_NUM\"]\n",
    "# df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "# cv = StratifiedGroupKFold(n_splits=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eed7be3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:21.095200Z",
     "iopub.status.busy": "2024-02-07T21:28:21.094816Z",
     "iopub.status.idle": "2024-02-07T21:28:22.748078Z",
     "shell.execute_reply": "2024-02-07T21:28:22.747011Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 1.664029,
     "end_time": "2024-02-07T21:28:22.750906",
     "exception": false,
     "start_time": "2024-02-07T21:28:21.086877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train[cat_cols] = df_train[cat_cols].astype(str)\n",
    "# import polars as pl\n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# # Fit Ordinal Encoder on Training Data\n",
    "# encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
    "# encoder.fit(df_train[cat_cols])\n",
    "\n",
    "# # Transform Training Data\n",
    "# df_train[cat_cols] = encoder.transform(df_train[cat_cols])\n",
    "# df_train[cat_cols] = df_train[cat_cols].fillna(-1)\n",
    "# df_train[cat_cols] = df_train[cat_cols].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cc4a29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:22.766849Z",
     "iopub.status.busy": "2024-02-07T21:28:22.766164Z",
     "iopub.status.idle": "2024-02-07T21:28:22.780619Z",
     "shell.execute_reply": "2024-02-07T21:28:22.779457Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.025248,
     "end_time": "2024-02-07T21:28:22.783159",
     "exception": false,
     "start_time": "2024-02-07T21:28:22.757911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_person_1_feats_1 = test_person_1.group_by(\"case_id\").agg(\n",
    "#     pl.col(\"mainoccupationinc_384A\").max().alias(\"mainoccupationinc_384A_max\"),\n",
    "#     (pl.col(\"incometype_1044T\") == \"SELFEMPLOYED\").max().alias(\"mainoccupationinc_384A_any_selfemployed\")\n",
    "# )\n",
    "\n",
    "# test_person_1_feats_2 = test_person_1.select([\"case_id\", \"num_group1\", \"housetype_905L\"]).filter(\n",
    "#     pl.col(\"num_group1\") == 0\n",
    "# ).drop(\"num_group1\").rename({\"housetype_905L\": \"person_housetype\"})\n",
    "\n",
    "# test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by(\"case_id\").agg(\n",
    "#     pl.col(\"pmts_pmtsoverdue_635A\").max().alias(\"pmts_pmtsoverdue_635A_max\"),\n",
    "#     (pl.col(\"pmts_dpdvalue_108P\") > 31).max().alias(\"pmts_dpdvalue_108P_over31\")\n",
    "# )\n",
    "\n",
    "# data_submission = test_basetable.join(\n",
    "#     test_static.select([\"case_id\"]+selected_static_cols), how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_static_cb.select([\"case_id\"]+selected_static_cb_cols), how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
    "# ).join(\n",
    "#     test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7195e82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:22.798812Z",
     "iopub.status.busy": "2024-02-07T21:28:22.798415Z",
     "iopub.status.idle": "2024-02-07T21:28:32.218372Z",
     "shell.execute_reply": "2024-02-07T21:28:32.217254Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 9.431294,
     "end_time": "2024-02-07T21:28:32.221469",
     "exception": false,
     "start_time": "2024-02-07T21:28:22.790175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "# case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
    "# case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
    "\n",
    "# cols_pred = []\n",
    "# for col in data.columns:\n",
    "#     if col[-1].isupper() and col[:-1].islower():\n",
    "#         cols_pred.append(col)\n",
    "\n",
    "# print(cols_pred)\n",
    "\n",
    "# def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "#     return (\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[[\"case_id\", \"WEEK_NUM\", \"target\"]].to_pandas(),\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "#         data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas()\n",
    "#     )\n",
    "\n",
    "# base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "# base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "# base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)\n",
    "\n",
    "# for df in [X_train, X_valid, X_test]:\n",
    "#     df = convert_strings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a275858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:32.238895Z",
     "iopub.status.busy": "2024-02-07T21:28:32.238439Z",
     "iopub.status.idle": "2024-02-07T21:28:32.246480Z",
     "shell.execute_reply": "2024-02-07T21:28:32.244638Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.019901,
     "end_time": "2024-02-07T21:28:32.249064",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.229163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Train: {X_train.shape}\")\n",
    "# print(f\"Valid: {X_valid.shape}\")\n",
    "# print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661f3324",
   "metadata": {
    "papermill": {
     "duration": 0.006806,
     "end_time": "2024-02-07T21:28:32.263196",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.256390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training LightGBM\n",
    "\n",
    "Minimal example of LightGBM training is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb1d2bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:28:32.278983Z",
     "iopub.status.busy": "2024-02-07T21:28:32.278570Z",
     "iopub.status.idle": "2024-02-07T21:29:52.839799Z",
     "shell.execute_reply": "2024-02-07T21:29:52.838506Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 80.572251,
     "end_time": "2024-02-07T21:29:52.842351",
     "exception": false,
     "start_time": "2024-02-07T21:28:32.270100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "# lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
    "\n",
    "# params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"metric\": \"auc\",\n",
    "#     \"max_depth\": 3,\n",
    "#     \"num_leaves\": 31,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"feature_fraction\": 0.9,\n",
    "#     \"bagging_fraction\": 0.8,\n",
    "#     \"bagging_freq\": 5,\n",
    "#     \"n_estimators\": 1000,\n",
    "#     \"verbose\": -1,\n",
    "# }\n",
    "\n",
    "# gbm = lgb.train(\n",
    "#     params,\n",
    "#     lgb_train,\n",
    "#     valid_sets=lgb_valid,\n",
    "#     callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e73d6",
   "metadata": {
    "papermill": {
     "duration": 0.008222,
     "end_time": "2024-02-07T21:29:52.859056",
     "exception": false,
     "start_time": "2024-02-07T21:29:52.850834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Evaluation with AUC and then comparison with the stability metric is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e5c4fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:29:52.878460Z",
     "iopub.status.busy": "2024-02-07T21:29:52.877756Z",
     "iopub.status.idle": "2024-02-07T21:30:15.482110Z",
     "shell.execute_reply": "2024-02-07T21:30:15.480907Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 22.617002,
     "end_time": "2024-02-07T21:30:15.484653",
     "exception": false,
     "start_time": "2024-02-07T21:29:52.867651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for base, X in [(base_train, X_train), (base_valid, X_valid), (base_test, X_test)]:\n",
    "#     y_pred = gbm.predict(X, num_iteration=gbm.best_iteration)\n",
    "#     base[\"score\"] = y_pred\n",
    "\n",
    "# print(f'The AUC score on the train set is: {roc_auc_score(base_train[\"target\"], base_train[\"score\"])}') \n",
    "# print(f'The AUC score on the valid set is: {roc_auc_score(base_valid[\"target\"], base_valid[\"score\"])}') \n",
    "# print(f'The AUC score on the test set is: {roc_auc_score(base_test[\"target\"], base_test[\"score\"])}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e10914f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:30:15.504198Z",
     "iopub.status.busy": "2024-02-07T21:30:15.503723Z",
     "iopub.status.idle": "2024-02-07T21:30:16.621500Z",
     "shell.execute_reply": "2024-02-07T21:30:16.620020Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 1.131134,
     "end_time": "2024-02-07T21:30:16.624526",
     "exception": false,
     "start_time": "2024-02-07T21:30:15.493392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "#     gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "#         .sort_values(\"WEEK_NUM\")\\\n",
    "#         .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "#         .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "#     x = np.arange(len(gini_in_time))\n",
    "#     y = gini_in_time\n",
    "#     a, b = np.polyfit(x, y, 1)\n",
    "#     y_hat = a*x + b\n",
    "#     residuals = y - y_hat\n",
    "#     res_std = np.std(residuals)\n",
    "#     avg_gini = np.mean(gini_in_time)\n",
    "#     return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std\n",
    "\n",
    "# stability_score_train = gini_stability(base_train)\n",
    "# stability_score_valid = gini_stability(base_valid)\n",
    "# stability_score_test = gini_stability(base_test)\n",
    "\n",
    "# print(f'The stability score on the train set is: {stability_score_train}') \n",
    "# print(f'The stability score on the valid set is: {stability_score_valid}') \n",
    "# print(f'The stability score on the test set is: {stability_score_test}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e23f05",
   "metadata": {
    "papermill": {
     "duration": 0.008408,
     "end_time": "2024-02-07T21:30:16.642325",
     "exception": false,
     "start_time": "2024-02-07T21:30:16.633917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission\n",
    "\n",
    "Scoring the submission dataset is below, we need to take care of new categories. Then we save the score as a last step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a30dccc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:30:16.662444Z",
     "iopub.status.busy": "2024-02-07T21:30:16.661989Z",
     "iopub.status.idle": "2024-02-07T21:30:16.775835Z",
     "shell.execute_reply": "2024-02-07T21:30:16.774344Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.127205,
     "end_time": "2024-02-07T21:30:16.778624",
     "exception": false,
     "start_time": "2024-02-07T21:30:16.651419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_submission = data_submission[cols_pred].to_pandas()\n",
    "# X_submission = convert_strings(X_submission)\n",
    "# categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
    "\n",
    "# for col in categorical_cols:\n",
    "#     train_categories = set(X_train[col].cat.categories)\n",
    "#     submission_categories = set(X_submission[col].cat.categories)\n",
    "#     new_categories = submission_categories - train_categories\n",
    "#     X_submission.loc[X_submission[col].isin(new_categories), col] = \"Unknown\"\n",
    "#     new_dtype = pd.CategoricalDtype(categories=train_categories, ordered=True)\n",
    "#     X_train[col] = X_train[col].astype(new_dtype)\n",
    "#     X_submission[col] = X_submission[col].astype(new_dtype)\n",
    "\n",
    "# y_submission_pred = gbm.predict(X_submission, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8664c2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-07T21:30:16.798722Z",
     "iopub.status.busy": "2024-02-07T21:30:16.798207Z",
     "iopub.status.idle": "2024-02-07T21:30:16.811051Z",
     "shell.execute_reply": "2024-02-07T21:30:16.809721Z"
    },
    "metadata": {},
    "papermill": {
     "duration": 0.026081,
     "end_time": "2024-02-07T21:30:16.813857",
     "exception": false,
     "start_time": "2024-02-07T21:30:16.787776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# submission = pd.DataFrame({\n",
    "#     \"case_id\": data_submission[\"case_id\"].to_numpy(),\n",
    "#     \"score\": y_submission_pred\n",
    "# }).set_index('case_id')\n",
    "# submission.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7493015,
     "sourceId": 50160,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 145.018038,
   "end_time": "2024-02-07T21:30:18.166484",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-07T21:27:53.148446",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
